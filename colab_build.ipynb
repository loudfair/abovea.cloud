{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Epstein Files Search Engine — Cloud Build\n",
        "\n",
        "This notebook downloads all data sources, normalizes them, and builds the search index entirely in Google Colab.\n",
        "The final index is saved to your Google Drive so you can pull it into the GitHub repo.\n",
        "\n",
        "**Runtime**: ~15 min | **RAM**: ~8GB peak | **Drive space**: ~600MB for index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Where the final index will be saved\n",
        "DRIVE_OUTPUT = '/content/drive/MyDrive/epstein-search-index'\n",
        "!mkdir -p \"$DRIVE_OUTPUT\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Clone the repo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!git clone https://github.com/loudfair/abovea.cloud.git /content/epstein-search\n",
        "%cd /content/epstein-search"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip install -q -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Download all data sources\n",
        "Downloads from HuggingFace, GitHub, and Archive.org — all in the cloud, nothing local."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%time\n",
        "import os\n",
        "os.chdir('/content/epstein-search')\n",
        "\n",
        "# ── Download HuggingFace datasets ──\n",
        "print(\"Downloading HuggingFace datasets...\")\n",
        "os.makedirs('downloads/huggingface', exist_ok=True)\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "hf_datasets = [\n",
        "    ('theelderemo/FULL_EPSTEIN_INDEX', 'full_index'),\n",
        "    ('to-be/epstein-emails', 'emails'),\n",
        "    ('svetfm/epstein-files-nov11-25-house-post-ocr-embeddings', 'embeddings'),\n",
        "    ('svetfm/epstein-fbi-files', 'fbi_files'),\n",
        "    ('vikash06/EpsteinFiles', 'fbi_ocr'),\n",
        "    ('567-labs/jmail-house-oversight', 'house_emails'),\n",
        "]\n",
        "\n",
        "for name, folder in hf_datasets:\n",
        "    outdir = f'downloads/huggingface/{folder}'\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    if os.path.exists(f'{outdir}/train.parquet'):\n",
        "        print(f'  ✓ {name} (cached)')\n",
        "        continue\n",
        "    try:\n",
        "        ds = load_dataset(name)\n",
        "        for split in ds:\n",
        "            ds[split].to_parquet(f'{outdir}/{split}.parquet')\n",
        "        print(f'  ✓ {name} ({len(ds[list(ds.keys())[0]])} rows)')\n",
        "    except Exception as e:\n",
        "        print(f'  ✗ {name}: {e}')\n",
        "\n",
        "# ── Clone GitHub repos ──\n",
        "print(\"\\nCloning GitHub repos...\")\n",
        "repos = [\n",
        "    (\"https://github.com/epstein-docs/epstein-docs.github.io.git\", \"epstein-docs\"),\n",
        "    (\"https://github.com/markramm/EpsteinFiles.git\", \"markramm\"),\n",
        "    (\"https://github.com/benbaessler/epfiles.git\", \"epfiles\"),\n",
        "    (\"https://github.com/theelderemo/FULL_EPSTEIN_INDEX.git\", \"full-index\"),\n",
        "    (\"https://github.com/HarleyCoops/TrumpEpsteinFiles.git\", \"trump-files\"),\n",
        "    (\"https://github.com/LMSBAND/epstein-files-db.git\", \"epstein-files-db\"),\n",
        "    (\"https://github.com/promexdotme/epstein-justice-files-text.git\", \"justice-files-text\"),\n",
        "    (\"https://github.com/phelix001/epstein-network.git\", \"epstein-network\"),\n",
        "    (\"https://github.com/maxandrews/Epstein-doc-explorer.git\", \"doc-explorer\"),\n",
        "    (\"https://github.com/yung-megafone/Epstein-Files.git\", \"magnet-links\"),\n",
        "    (\"https://github.com/SvetimFM/epstein-files-visualizations.git\", \"visualizations\"),\n",
        "    (\"https://github.com/paulgp/epstein-document-search.git\", \"document-search\"),\n",
        "]\n",
        "for url, dirname in repos:\n",
        "    dest = f\"downloads/github/{dirname}\"\n",
        "    if os.path.isdir(dest):\n",
        "        print(f\"  ✓ {dirname} (cached)\")\n",
        "    else:\n",
        "        r = os.system(f'git clone --depth 1 -q \"{url}\" \"{dest}\" 2>/dev/null')\n",
        "        print(f\"  {'✓' if r == 0 else '✗'} {dirname}\")\n",
        "\n",
        "# ── Download Archive.org PDFs ──\n",
        "print(\"\\nDownloading Archive.org files...\")\n",
        "archives = [\n",
        "    (\"downloads/archive/flight-logs/epstein-flight-logs.pdf\",\n",
        "     \"https://archive.org/download/epstein-flight-logs-unredacted-17/EPSTEIN%20FLIGHT%20LOGS%20UNREDACTED%20%2817%29.pdf\",\n",
        "     \"Flight logs\"),\n",
        "    (\"downloads/archive/black-book/black-book.pdf\",\n",
        "     \"https://archive.org/download/jeffrey-epstein-39s-little-black-book-unredacted/Jeffrey%20Epstein%27s%20Little%20Black%20Book%20unredacted.pdf\",\n",
        "     \"Black book\"),\n",
        "    (\"downloads/archive/epstein-docs-collection/Epstein-Docs.pdf\",\n",
        "     \"https://ia600705.us.archive.org/21/items/epsteindocs/Epstein-Docs.pdf\",\n",
        "     \"Epstein docs collection\"),\n",
        "    (\"downloads/archive/depositions/Edwards-vs-Epstein-depositions.pdf\",\n",
        "     \"https://ia600705.us.archive.org/21/items/epsteindocs/12%23%20Epstein%20deposition%27s%20-%20Edwards%20vs%20Epstein%20%2B%20attachments.pdf\",\n",
        "     \"Depositions\"),\n",
        "]\n",
        "for path, url, label in archives:\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    if os.path.exists(path):\n",
        "        print(f\"  ✓ {label} (cached)\")\n",
        "    else:\n",
        "        r = os.system(f'curl -sL -o \"{path}\" \"{url}\"')\n",
        "        print(f\"  {'✓' if r == 0 else '✗'} {label}\")\n",
        "\n",
        "# ── Normalize ──\n",
        "print(\"\\n\\nNormalizing all sources...\")\n",
        "os.makedirs('data/normalized', exist_ok=True)\n",
        "os.makedirs('data/index', exist_ok=True)\n",
        "!python normalize.py\n",
        "\n",
        "# ── Build index ──\n",
        "print(\"\\nBuilding search index...\")\n",
        "!python build_index.py\n",
        "\n",
        "print(\"\\n✅ Done!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Check disk & memory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import psutil, shutil\n",
        "\n",
        "mem = psutil.virtual_memory()\n",
        "disk = shutil.disk_usage('/content')\n",
        "\n",
        "print(f\"RAM:  {mem.available / (1024**3):.1f} GB available / {mem.total / (1024**3):.1f} GB total\")\n",
        "print(f\"Disk: {disk.free / (1024**3):.1f} GB available / {disk.total / (1024**3):.1f} GB total\")\n",
        "print()\n",
        "!du -sh /content/epstein-search/downloads/ 2>/dev/null || echo 'No downloads yet'\n",
        "!du -sh /content/epstein-search/data/ 2>/dev/null || echo 'No data yet'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Run security audit"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!python audit.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Copy index to Google Drive\n",
        "Copies the built search index to your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import shutil, os\n",
        "\n",
        "INDEX_DIR = '/content/epstein-search/data/index'\n",
        "\n",
        "# Copy index files to Drive\n",
        "for f in os.listdir(INDEX_DIR):\n",
        "    src = os.path.join(INDEX_DIR, f)\n",
        "    dst = os.path.join(DRIVE_OUTPUT, f)\n",
        "    print(f'Copying {f} ({os.path.getsize(src) / (1024*1024):.1f} MB)...')\n",
        "    shutil.copy2(src, dst)\n",
        "\n",
        "# Also copy the corpus for the web UI\n",
        "corpus_src = '/content/epstein-search/data/normalized/corpus.jsonl'\n",
        "if os.path.exists(corpus_src):\n",
        "    corpus_dst = os.path.join(DRIVE_OUTPUT, 'corpus.jsonl')\n",
        "    print(f'Copying corpus.jsonl ({os.path.getsize(corpus_src) / (1024*1024):.1f} MB)...')\n",
        "    shutil.copy2(corpus_src, corpus_dst)\n",
        "\n",
        "print(f'\\n✓ Index saved to Google Drive: {DRIVE_OUTPUT}')\n",
        "!du -sh \"$DRIVE_OUTPUT\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Create GitHub Release (optional)\n",
        "Uploads the index as a GitHub release so the repo can download it directly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Uncomment and set your GitHub token to upload as a release\n",
        "# GITHUB_TOKEN = 'ghp_...'  # paste your token here\n",
        "#\n",
        "# !pip install -q requests\n",
        "# import requests, os, json\n",
        "#\n",
        "# REPO = 'loudfair/abovea.cloud'\n",
        "# TAG = 'index-v1'\n",
        "# headers = {'Authorization': f'token {GITHUB_TOKEN}'}\n",
        "#\n",
        "# # Create release\n",
        "# r = requests.post(\n",
        "#     f'https://api.github.com/repos/{REPO}/releases',\n",
        "#     headers=headers,\n",
        "#     json={'tag_name': TAG, 'name': 'Search Index v1', 'body': 'Pre-built search index'}\n",
        "# )\n",
        "# release = r.json()\n",
        "# upload_url = release['upload_url'].replace('{?name,label}', '')\n",
        "#\n",
        "# # Tar the index\n",
        "# !cd /content/epstein-search/data && tar czf /tmp/search-index.tar.gz index/\n",
        "#\n",
        "# # Upload\n",
        "# with open('/tmp/search-index.tar.gz', 'rb') as f:\n",
        "#     r = requests.post(\n",
        "#         f'{upload_url}?name=search-index.tar.gz',\n",
        "#         headers={**headers, 'Content-Type': 'application/gzip'},\n",
        "#         data=f\n",
        "#     )\n",
        "# print(f'✓ Uploaded: {r.json().get(\"browser_download_url\", r.text)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Quick test — verify the index works"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!python search.py --text \"flight log\" --limit 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Next steps\n",
        "\n",
        "Your index is now in Google Drive at `MyDrive/epstein-search-index/`.\n",
        "\n",
        "### Pull to your local repo with rclone (recommended)\n",
        "\n",
        "```bash\n",
        "# One-time: install rclone and connect Google Drive\n",
        "brew install rclone        # macOS (or: curl https://rclone.org/install.sh | sudo bash)\n",
        "rclone config              # → New remote → name it \"gdrive\" → Google Drive → follow auth\n",
        "\n",
        "# Then just run the setup script:\n",
        "git clone https://github.com/loudfair/abovea.cloud.git\n",
        "cd abovea.cloud\n",
        "chmod +x setup_from_drive.sh\n",
        "./setup_from_drive.sh      # syncs index from Drive → ready in seconds\n",
        "\n",
        "# Start the web UI:\n",
        "source venv/bin/activate\n",
        "python app.py              # → http://localhost:5000\n",
        "```\n",
        "\n",
        "### Or mount Drive directly (zero download)\n",
        "```bash\n",
        "# Mount your Drive folder as a local directory — files stream on demand\n",
        "mkdir -p data/index\n",
        "rclone mount gdrive:epstein-search-index/ data/index/ --vfs-cache-mode full --daemon\n",
        "# Now search.py and app.py just work — reads stream from Drive\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Epstein Search — Cloud Build",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}